steps:
# Step 1: Get the DAGs folder GCS path, clean it, and write to a file
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'get_dags_bucket'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gcloud composer environments describe tu-main-data-airflow-v1 \
      --location asia-northeast3 \
      --format="get(config.dagGcsPrefix)" | tr -d '\r\n' > /workspace/dags_path.txt
    echo "Found and cleaned DAGs bucket path: $(cat /workspace/dags_path.txt)"

# Step 2: Use gsutil rsync to sync the dags folder within a shell.
# The -d flag deletes files in the destination that are not in the source.
- name: 'gcr.io/cloud-builders/gsutil'
  id: 'sync_dags'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gsutil rsync -d -r dags/ "$(cat /workspace/dags_path.txt)"

# Step 3: Update the PyPI packages from requirements.txt
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'update_dependencies'
  args:
  - 'composer'
  - 'environments'
  - 'update'
  - 'tu-main-data-airflow-v1'
  - '--location'
  - 'asia-northeast3'
  - '--update-pypi-packages-from-file'
  - 'requirements.txt'

options:
  logging: CLOUD_LOGGING_ONLY